accum_steps: 8
allow_tf32: true
attn_backend: flash_attn_2
batch_size: 768

# ---- MODEL SIZE (LARGE VERSION) ----
d_lex: 384          # was 192
d_lex_emb: 1024     # was 512
rank: 512           # was 256
heads: 8           # was 8

dev_eval_every_epochs: 0.2
dev_eval_sample: 800
dev_tsv: null
dtype: bf16
early_stop_min_delta: 0.0001
early_stop_patience: 3
ema: 0.999
epochs: 20
export_after_train: false
export_demo_query: what energy is used in photosynthesis
export_demo_topk: 5
export_file: null
export_from: train
export_include_negs: true
export_out_dir: null
faiss_hnsw_efc: 200
faiss_hnsw_m: 64
faiss_search_efs: 256
fp16: false
grad_ckpt: true
inbatch_negs: false
lambda_distill: 0.0
lambda_ent: 0.0015
lambda_lex: 0.25
lambda_ortho: 0.001
lambda_ret: 1.0
late_interaction: true
log_every: 50
lr: 0.0002
lr_schedule: cosine
m_teacher: 768
margin_lex: 0.25
max_grad_norm: 1.0
max_len: 512
max_train_rows: null
neg_per_sample: 7
num_workers: 2
output_dir: runs/allminilm
resume: null
seed: 42
shuffle_before_split: true
teacher: sentence-transformers/all-MiniLM-L6-v2
teacher_bs: 256
temp: 0.07
tokenizer: sentence-transformers/all-MiniLM-L6-v2
topk_d: 1
topk_q: 4
torch_compile: false
train_tsv: data/train.tsv
val_ratio: 0.02
wandb: false
# wandb_mode: online
# wandb_project: longmatrix
# wandb_run: allminilm-large
wandb_table_samples: 8
warmup_steps: 1500
weight_decay: 0.01